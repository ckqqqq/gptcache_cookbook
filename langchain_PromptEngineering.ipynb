{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions tell the model what to do, how to use external information if provided, what to do with the query, and how to construct the output.\n",
    "\n",
    "External information or context(s) act as an additional ***source** of knowledge for the model. These can be manually inserted into the prompt, retrieved via a vector database (retrieval augmentation), or pulled in via other means (APIs, calculations, etc.).\n",
    "\n",
    "User input or query is typically (but not always) a query input into the system by a human user (the prompter).\n",
    "\n",
    "Output indicator marks the beginning of the to-be-generated text. If generating Python code, we may use import to indicate to the model that it must begin writing Python code (as most Python scripts begin with import)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt可以分成四个部分\n",
    "* INSTRUCTIONS\n",
    "\n",
    "\"MAnswer the question based on the context below.If the\n",
    "question cannot be answered using the information provided answer\n",
    "th\"don't know”。\n",
    "\n",
    "* CONTEXTS 外部信息(EXTERNAL INFO)\n",
    "\n",
    "Context:\n",
    "Large Language Models (LLMS)are the latest models used in NLP.Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications.These models can be accessed via Hugging Face's \"transformers'library,via Open4Iusing the \"openai\" library and via Cohere using the \"cohere\"library.\n",
    "\n",
    "* Question 用户提问\n",
    "\n",
    "Question:Which libraries and model providers offer LLMs?\n",
    "\n",
    "* 回答\n",
    "\n",
    "Answer: XXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with  \"I don't know.\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. Their superior performance over smaller models has mode them incredibly useful for developers building  NLP enabled applications. These models can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: Which libraries and mode providers offer LLMS?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"]=\"127.0.0.1:11137\"\n",
    "os.environ[\"https_proxy\"]=\"127.0.0.1:11137\"\n",
    "os.environ['OPENAI_API_KEY']=\"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLMs can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "#Initialize\n",
    "openai=OpenAI(\n",
    "    model_name=\"text-davinci-003\"\n",
    ")\n",
    "print(openai(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Templates\n",
    "就是将字符做了封装\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "template=\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with  \"I don't know.\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. Their superior performance over smaller models has mode them incredibly useful for developers building  NLP enabled applications. These models can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt_template=PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")\n",
    "query=\"Which libraries and mode providers offer LLMS?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below. If the question cannot be answered using the information provided answer with  \"I don't know.\n",
      "\n",
      "Context: Large Language Models (LLMs) are the latest models used in NLP. Their superior performance over smaller models has mode them incredibly useful for developers building  NLP enabled applications. These models can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\n",
      "\n",
      "Question: Which libraries and model provides offer LLMs?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    prompt_template.format(\n",
    "        query=\"Which libraries and model provides offer LLMs?\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few shot 形式的 Template\n",
    "Parametric knowledge:参数知识 the knowledge mentioned above is anything that has been leared by the model during training time and is stored within the model weights\n",
    "Source Knowledge: any knowledge provided to the modes at inference time vie the input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### few shot\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# examples\n",
    "examples =[\n",
    "    {\n",
    "        \"query\":\"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\":\"What time is it?\",\n",
    "        \"answer\":\"It's time to get a watch.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# example template\n",
    "example_template=\"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt= PromptTemplate(\n",
    "    input_variables=[\"query\",\"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix=\"\"\"The following are exerpts from conversations with an AI assistent. The assistant is typically sarcastic and witty, producing creative and funny response to the users questions. Here are some examples:\n",
    "\"\"\"\n",
    "\n",
    "suffix=\"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "few_shot_prompt_template=FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### openai的容量限制\n",
    "** context window=input tokens + output tokens **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
